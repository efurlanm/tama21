{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santos Dumont (SD) - Numba GPU B715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sd_numba_gpu.py\n",
    "import numpy as np, math\n",
    "from time import time\n",
    "from mpi4py import MPI\n",
    "from numba import cuda, njit, prange, config\n",
    "\n",
    "# parameters\n",
    "n            = 4800    # n x n grid\n",
    "energy       = 1.0     # energy to be injected per iteration\n",
    "niters       = 500     # number of iterations\n",
    "# initialize three heat sources\n",
    "nsources     = 3       # number of sources of energy\n",
    "sources      = np.zeros((nsources, 2), np.int16)\n",
    "sources[:,:] = [ [n//2, n//2], [n//3, n//3], [n*4//5, n*8//9] ]\n",
    "# initialize the data arrays\n",
    "anew         = np.zeros((n + 2, n + 2), np.float64)\n",
    "aold         = np.zeros((n + 2, n + 2), np.float64)\n",
    "\n",
    "# configure blocks & grids\n",
    "## set the number of threads in a block\n",
    "threads_per_block = (16, 16)    # based on trial and error\n",
    "## calculate the number of thread blocks in the grid\n",
    "blocks_per_grid_x = math.ceil(aold.shape[0] / threads_per_block[0])\n",
    "blocks_per_grid_y = math.ceil(aold.shape[1] / threads_per_block[1])\n",
    "blocks_per_grid   = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "\n",
    "# computationally intensive core\n",
    "@cuda.jit\n",
    "def kernel(A, B):\n",
    "    n = A.shape[0] - 1\n",
    "    i, j = cuda.grid(2)\n",
    "    if (i > 0 and j > 0) and (i < n and j < n) :\n",
    "        A[i,j]=B[i,j]*.5+(B[i-1,j]+B[i+1,j]+B[i,j-1]+B[i,j+1])*.125\n",
    "\n",
    "# start of main routine\n",
    "\n",
    "#---mpi4py---\n",
    "comm  = MPI.COMM_WORLD            # MPI default communicator\n",
    "size  = comm.Get_size()           # MPI size\n",
    "rank  = comm.Get_rank()           # MPI rank\n",
    "name  = MPI.Get_processor_name()  # core hostname (eg sdumont3170)\n",
    "\n",
    "#Only 2 processes per node are selected via Slurm. Within a node, color \n",
    "#rank 0 corresponds to the first process of this node, and color rank 1 \n",
    "#corresponds to the second process of this node, and the other nodes are \n",
    "#similar. Example:\n",
    "#  node      rank  color rank\n",
    "#----------- ----  ----------\n",
    "#sdumont3170   0        0\n",
    "#sdumont3170   1        1\n",
    "#sdumont3171   2        0\n",
    "#sdumont3171   3        1\n",
    "#sdumont3172   4        0\n",
    "#sdumont3172   5        1\n",
    "#sdumont3173   6        0\n",
    "#sdumont3173   7        1\n",
    "for i, c in enumerate(name) :     # find first digit in hostname\n",
    "    if c.isdigit() :\n",
    "        break\n",
    "mcol  = int(name[i:])             # extract number from hostname\n",
    "scomm = comm.Split(color = mcol)  # new communicator for the node\n",
    "crank = scomm.Get_rank()          # get the node color rank\n",
    "\n",
    "#---numba.cuda---\n",
    "#In this implementation, Slurm is configured to run only 2 processes on \n",
    "#each node. For each of these processes (cores), a single GPU is \n",
    "#associated. Thus, within a node, color rank 0 is associated with GPU 0, \n",
    "#and color rank 1 is associated with GPU 1.\n",
    "cuda.select_device(crank)         # 'color rank' 0 = 'gpu id' 0, etc.\n",
    "cid = cuda.current_context().device.id\n",
    "\n",
    "# time measurement for rank 0\n",
    "if not rank :\n",
    "    tt = -time()    # rank 0 time\n",
    "    tk = 0          # accumulate kernel time\n",
    "    tc = 0          # accumulate communication time\n",
    "\n",
    "# determine my coordinates (x,y)\n",
    "pdims = MPI.Compute_dims(size, 2)\n",
    "px    = pdims[0]\n",
    "py    = pdims[1]\n",
    "rx    = rank % px\n",
    "ry    = rank // px\n",
    "\n",
    "# determine my four neighbors\n",
    "north = (ry - 1) * px + rx\n",
    "if (ry - 1) < 0 :\n",
    "    north = MPI.PROC_NULL\n",
    "south = (ry + 1) * px + rx\n",
    "if (ry + 1) >= py :\n",
    "    south = MPI.PROC_NULL\n",
    "west = ry * px + rx - 1\n",
    "if (rx - 1) < 0 :\n",
    "    west = MPI.PROC_NULL\n",
    "east = ry * px + rx + 1\n",
    "if (rx + 1) >= px :\n",
    "    east = MPI.PROC_NULL\n",
    "\n",
    "# decompose the domain\n",
    "bx   = n // px          # block size in x\n",
    "by   = n // py          # block size in y\n",
    "offx = rx * bx + 1      # offset in x\n",
    "offy = ry * by + 1      # offset in y\n",
    "\n",
    "# sources in my area, local to my rank\n",
    "locnsources = 0\n",
    "locsources  = np.empty((nsources, 2), np.int16)\n",
    "\n",
    "# determine which sources are in my patch\n",
    "for i in range(nsources) :\n",
    "    locx = sources[i, 0] - offx\n",
    "    locy = sources[i, 1] - offy\n",
    "    if(locx >= 0 and locx <= bx and locy >= 0 and locy <= by) :\n",
    "        locsources[locnsources, 0] = locx\n",
    "        locsources[locnsources, 1] = locy\n",
    "        locnsources += 1\n",
    "\n",
    "# working arrays with 1-wide halo zones\n",
    "anew = np.zeros((bx+2, by+2), np.float64)\n",
    "aold = np.zeros((bx+2, by+2), np.float64)\n",
    "\n",
    "# system total heat\n",
    "rheat = np.zeros(1, np.float64)\n",
    "bheat = np.zeros(1, np.float64)\n",
    "\n",
    "# copy the first arrays to the device\n",
    "if not rank : tc -= time()\n",
    "anew_global_mem    = cuda.to_device(anew)\n",
    "aold_global_mem    = cuda.to_device(aold)\n",
    "if not rank : tc += time()\n",
    "   \n",
    "# main loop\n",
    "for _ in range(0, niters, 2) :\n",
    "\n",
    "    # exchange data with neighbors\n",
    "    if north != MPI.PROC_NULL :\n",
    "        r1=comm.irecv(source=north, tag=1)\n",
    "        s1=comm.isend(aold[1, 1:bx+1], dest=north, tag=1)\n",
    "    if south != MPI.PROC_NULL :\n",
    "        r2=comm.irecv(source=south, tag=1)\n",
    "        s2=comm.isend(aold[bx, 1:bx+1], dest=south, tag=1)\n",
    "    if east != MPI.PROC_NULL :\n",
    "        r3 = comm.irecv(source=east, tag=1)\n",
    "        s3 = comm.isend(aold[1:bx+1, bx], dest=east, tag=1)\n",
    "    if west != MPI.PROC_NULL :\n",
    "        r4 = comm.irecv(source=west, tag=1)\n",
    "        s4 = comm.isend(aold[1:bx+1, 1], dest=west, tag=1)\n",
    "    # wait for the end of communication\n",
    "    if north != MPI.PROC_NULL :\n",
    "        s1.wait()\n",
    "        aold[0, 1:bx+1] = r1.wait()\n",
    "    if south != MPI.PROC_NULL :\n",
    "        s2.wait()\n",
    "        aold[bx+1, 1:bx+1] = r2.wait()\n",
    "    if east != MPI.PROC_NULL :\n",
    "        s3.wait()\n",
    "        aold[1:bx+1, bx+1] = r3.wait()\n",
    "    if west != MPI.PROC_NULL :\n",
    "        s4.wait\n",
    "        aold[1:bx+1, 0] = r4.wait()\n",
    "\n",
    "    # copy the received array to the device\n",
    "    if not rank : tc -= time()\n",
    "    aold_global_mem = cuda.to_device(aold)\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # update grid\n",
    "    if not rank : tk -= time()\n",
    "    kernel[blocks_per_grid, threads_per_block](\n",
    "        anew_global_mem, aold_global_mem)\n",
    "    if not rank : tk += time()\n",
    "        \n",
    "    # copy the result back to the host\n",
    "    if not rank : tc -= time()\n",
    "    anew = anew_global_mem.copy_to_host()\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # refresh heat sources\n",
    "    for i in range(locnsources) :\n",
    "        anew[locsources[i, 0]-1, locsources[i, 1]-1] += energy\n",
    "\n",
    "    # exchange data with neighbors\n",
    "    if north != MPI.PROC_NULL :\n",
    "        r1=comm.irecv(source=north, tag=1)\n",
    "        s1=comm.isend(anew[1, 1:bx+1], dest=north, tag=1)\n",
    "    if south != MPI.PROC_NULL :\n",
    "        r2=comm.irecv(source=south, tag=1)\n",
    "        s2=comm.isend(anew[bx, 1:bx+1], dest=south, tag=1)\n",
    "    if east != MPI.PROC_NULL :\n",
    "        r3 = comm.irecv(source=east, tag=1)\n",
    "        s3 = comm.isend(anew[1:bx+1, bx], dest=east, tag=1)\n",
    "    if west != MPI.PROC_NULL :\n",
    "        r4 = comm.irecv(source=west, tag=1)\n",
    "        s4 = comm.isend(anew[1:bx+1, 1], dest=west, tag=1)\n",
    "    # wait for the end of communication\n",
    "    if north != MPI.PROC_NULL :\n",
    "        s1.wait()\n",
    "        anew[0, 1:bx+1] = r1.wait()\n",
    "    if south != MPI.PROC_NULL :\n",
    "        s2.wait()\n",
    "        anew[bx+1, 1:bx+1] = r2.wait()\n",
    "    if east != MPI.PROC_NULL :\n",
    "        s3.wait()\n",
    "        anew[1:bx+1, bx+1] = r3.wait()\n",
    "    if west != MPI.PROC_NULL :\n",
    "        s4.wait\n",
    "        anew[1:bx+1, 0] = r4.wait()\n",
    "\n",
    "    # copy the received array to the device\n",
    "    if not rank : tc -= time()\n",
    "    anew_global_mem = cuda.to_device(anew)\n",
    "    if not rank : tc += time()\n",
    "\n",
    "    # update grid\n",
    "    if not rank : tk -= time()\n",
    "    kernel[blocks_per_grid, threads_per_block](\n",
    "        aold_global_mem, anew_global_mem)\n",
    "    if not rank : tk += time()\n",
    "        \n",
    "    # copy the result back to the host\n",
    "    if not rank : tc -= time()\n",
    "    aold = aold_global_mem.copy_to_host()\n",
    "    if not rank : tc += time()\n",
    "        \n",
    "    # refresh heat sources\n",
    "    for i in range(locnsources) :\n",
    "        aold[locsources[i, 0]-1, locsources[i, 1]-1] += energy \n",
    "\n",
    "# end for\n",
    "\n",
    "# get final heat in the system\n",
    "bheat[0] = np.sum(aold[1:-1, 1:-1])\n",
    "comm.Reduce(bheat, rheat)\n",
    "\n",
    "# show the result\n",
    "print(f\"3. {name:11s}   {rank:02d}    {crank:02d}   {cid:02d}\")\n",
    "if not rank :\n",
    "    tt += time()\n",
    "    print( \"1. hostname    rank crank  cid\")\n",
    "    print( \"2. ----------- ---- ----- ----\")\n",
    "    print( \"4. ---------------------------\")\n",
    "    print(f\"5. Heat:{rheat[0]:.4f}\", end=\", \")\n",
    "    print(f\"TT:{tt:.4f}\", end=\", \")\n",
    "    print(f\"KT:{tk:.4f}\", end=\", \")\n",
    "    print(f\"CT:{tc:.4f}\", end=\", \")\n",
    "    print(f\"MPI:{size}\", end=\", \")\n",
    "    print(f\"dim:{n}\", end=\", \")\n",
    "    print(f\"ite:{niters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A fila *nvidia_dev* tem 4 nós, cada nó com 2 GPUs, totalizando 8 GPUs\n",
    "* Em cada nó, ao utilizar as 2 GPUs de forma exclusiva, 22 CPUs ficam sem uso\n",
    "* 2 CPUs são utilizadas, uma para cada GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp sd_numba_gpu.py /scratch${PWD#/prj}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ng2.srm\n"
     ]
    }
   ],
   "source": [
    "%%writefile sd_numba_gpu.srm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name sd_numba_gpu  # Job name\n",
    "#SBATCH --partition nvidia_dev   # Select partition\n",
    "#SBATCH --ntasks-per-node=2      # Tasks per node\n",
    "#SBATCH --nodes=2                # Minimum to be allocated\n",
    "#SBATCH --ntasks=4               # Total tasks\n",
    "#SBATCH --time=00:20:00          # Limit execution time\n",
    "#SBATCH --exclusive              # Exclusive acccess to nodes\n",
    "\n",
    "echo '========================================'\n",
    "echo '- Job ID:' $SLURM_JOB_ID\n",
    "echo '- Tasks per node:' $SLURM_NTASKS_PER_NODE\n",
    "echo '- # of nodes in the job:' $SLURM_JOB_NUM_NODES\n",
    "echo '- # of tasks:' $SLURM_NTASKS\n",
    "echo '- Dir from which sbatch was invoked:' ${SLURM_SUBMIT_DIR##*/}\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "echo -n '- List of nodes allocated to the job: '\n",
    "nodeset -e $SLURM_JOB_NODELIST\n",
    "\n",
    "# Environment\n",
    "echo '-- modules ----------------------------'\n",
    "echo 'conda activate env2, --stack env3'\n",
    "cd\n",
    "SCR=/scratch${PWD#/prj}\n",
    "cd $SCR\n",
    "source $SCR/env2/etc/profile.d/conda.sh\n",
    "conda activate $SCR/env2\n",
    "conda activate --stack $SCR/env3\n",
    "cd $SCR/b715\n",
    "\n",
    "# Executable\n",
    "EXEC=\"python sd_numba_gpu.py\"\n",
    "\n",
    "# Start\n",
    "OPT='--mpi=pmi2 --cpu_bind=cores --distribution=block:cyclic'\n",
    "echo '-- run --------------------------------'\n",
    "echo '$ srun -n' $SLURM_NTASKS ${EXEC##*/}\n",
    "echo '-- output -----------------------------'\n",
    "srun  $OPT  -n $SLURM_NTASKS  $EXEC  | sort\n",
    "echo '~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:6px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 processo, 1 GPU, 1 nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3611551\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=1  --nodes=1  --ntasks=1 sd_numba_gpu.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME    JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-11-07T11:50:38  3611551  nvidia_dev   ng2   R  0:05     1   24\n"
     ]
    }
   ],
   "source": [
    "! squeue --name sd_numba_gpu --partition=nvidia_dev --format \"%.19S  %.7i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME    JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name sd_numba_gpu --partition=nvidia_dev --format \"%.19S  %.7i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 3611551\n",
      "- Tasks per node: 1\n",
      "- # of nodes in the job: 1\n",
      "- # of tasks: 1\n",
      "- Dir from which sbatch was invoked: 2021-11-08\n",
      "- List of nodes allocated to the job: sdumont3052\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 1 python ng2.py\n",
      "-- output -----------------------------\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3052   00    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:105.8648, KT:2.6595, CT:103.1252, MPI:1, dim:4800, ite:500\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-3611551.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:6px;border-width:0;color:gray;background-color:gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8 processos, 8 GPUs, 4 nós"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3614181\n"
     ]
    }
   ],
   "source": [
    "! sbatch  --ntasks-per-node=2  --nodes=4  sd_numba_gpu.srm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME    JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n",
      "2021-11-07T11:53:45  3614181  nvidia_dev   ng2   R  0:01     4   96\n"
     ]
    }
   ],
   "source": [
    "! squeue --name sd_numba_gpu --format \"%.19S  %.7i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         START_TIME    JOBID   PARTITION  NAME  ST  TIME NODES CPUS\n"
     ]
    }
   ],
   "source": [
    "! squeue --name sd_numba_gpu --format \"%.19S  %.7i  %.10P %.5j  %.2t %.5M %.5D %.4C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "- Job ID: 3614181\n",
      "- Tasks per node: 2\n",
      "- # of nodes in the job: 4\n",
      "- # of tasks: 4\n",
      "- Dir from which sbatch was invoked: 2021-11-08\n",
      "- List of nodes allocated to the job: sdumont3052 sdumont3053 sdumont3054 sdumont3055\n",
      "-- modules ----------------------------\n",
      "conda activate env2, --stack env3\n",
      "-- run --------------------------------\n",
      "$ srun -n 4 python ng2.py\n",
      "-- output -----------------------------\n",
      "srun: Warning: can't honor --ntasks-per-node set to 2 which doesn't match the requested tasks 4 with the number of requested nodes 4. Ignoring --ntasks-per-node.\n",
      "1. hostname    rank crank  cid\n",
      "2. ----------- ---- ----- ----\n",
      "3. sdumont3052   00    00   00\n",
      "3. sdumont3053   01    00   00\n",
      "3. sdumont3054   02    00   00\n",
      "3. sdumont3055   03    00   00\n",
      "4. ---------------------------\n",
      "5. Heat:1500.0000, TT:27.8101, KT:0.7045, CT:26.7243, MPI:4, dim:4800, ite:500\n",
      "~~ end ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "! cat /scratch${PWD#/prj}/slurm-3614181.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
